{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T05:55:44.445599Z",
     "iopub.status.busy": "2025-05-01T05:55:44.445317Z",
     "iopub.status.idle": "2025-05-01T05:56:50.600735Z",
     "shell.execute_reply": "2025-05-01T05:56:50.600075Z",
     "shell.execute_reply.started": "2025-05-01T05:55:44.445578Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fashion-MNIST CNN Classifier - Improved Version\n",
    "===============================================\n",
    "This notebook implements an improved CNN model for Fashion-MNIST classification\n",
    "with better code organization, data augmentation, learning rate scheduling,\n",
    "early stopping, and comprehensive visualizations.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# 1. IMPORTS AND CONFIGURATION\n",
    "# ============================================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    \"\"\"Configuration class for hyperparameters and paths\"\"\"\n",
    "    # Data paths (configurable for different environments)\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    TRAIN_CSV = DATA_DIR / \"fashion-mnist_train.csv\"\n",
    "    TEST_CSV = DATA_DIR / \"fashion-mnist_test.csv\"\n",
    "    \n",
    "    # Alternative: Kaggle paths (uncomment if running on Kaggle)\n",
    "    # TRAIN_CSV = Path(\"/kaggle/input/fashion-mnist/fashion-mnist_train.csv\")\n",
    "    # TEST_CSV = Path(\"/kaggle/input/fashion-mnist/fashion-mnist_test.csv\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "    EPOCHS = 50\n",
    "    VALIDATION_SPLIT = 0.1\n",
    "    DROPOUT_RATE = 0.5\n",
    "    WEIGHT_DECAY = 1e-4\n",
    "    \n",
    "    # Training settings\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    MIN_DELTA = 0.001\n",
    "    NUM_WORKERS = 0  # Set to 0 on Windows to avoid multiprocessing issues\n",
    "    PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "    \n",
    "    # Model settings\n",
    "    SAVE_BEST_MODEL = True\n",
    "    MODEL_SAVE_PATH = Path(\"./models\")\n",
    "    MODEL_SAVE_PATH.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Class names for Fashion-MNIST\n",
    "    CLASS_NAMES = [\n",
    "        'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "    ]\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "config = Config()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-24T05:26:16.900140Z",
     "iopub.status.busy": "2025-04-24T05:26:16.899425Z",
     "iopub.status.idle": "2025-04-24T05:27:29.165453Z",
     "shell.execute_reply": "2025-04-24T05:27:29.164674Z",
     "shell.execute_reply.started": "2025-04-24T05:26:16.900117Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Train file not found. Using test file for training.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find CSV file: /kaggle/input/fashion-mnist/fashion-mnist_test.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 20\u001b[0m, in \u001b[0;36mFashionMNISTDataset.__init__\u001b[1;34m(self, csv_file, transform, normalize)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Fallback for Kaggle environment\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m         handle,\n\u001b[0;32m    875\u001b[0m         ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m         newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m     )\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/fashion-mnist/fashion-mnist_test.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_loader, val_loader, test_loader\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m load_data(config)\n",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    150\u001b[0m     test_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/fashion-mnist/fashion-mnist_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m FashionMNISTDataset(\n\u001b[0;32m    154\u001b[0m     train_path, \n\u001b[0;32m    155\u001b[0m     transform\u001b[38;5;241m=\u001b[39mget_transforms(augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    156\u001b[0m     normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Normalization handled in transforms\u001b[39;00m\n\u001b[0;32m    157\u001b[0m )\n\u001b[0;32m    159\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m FashionMNISTDataset(\n\u001b[0;32m    160\u001b[0m     test_path,\n\u001b[0;32m    161\u001b[0m     transform\u001b[38;5;241m=\u001b[39mget_transforms(augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    162\u001b[0m     normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    163\u001b[0m )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Split training data into train and validation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 30\u001b[0m, in \u001b[0;36mFashionMNISTDataset.__init__\u001b[1;34m(self, csv_file, transform, normalize)\u001b[0m\n\u001b[0;32m     28\u001b[0m         data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find CSV file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find CSV file: /kaggle/input/fashion-mnist/fashion-mnist_test.csv"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. DATASET LOADER WITH PROPER NORMALIZATION\n",
    "# ============================================================================\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Improved Fashion-MNIST dataset loader with proper normalization.\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to CSV file\n",
    "        transform: Optional transform to be applied on images\n",
    "        normalize: Whether to normalize pixel values (default: True)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        csv_file: str, \n",
    "        transform: Optional[transforms.Compose] = None,\n",
    "        normalize: bool = True\n",
    "    ):\n",
    "        try:\n",
    "            data = pd.read_csv(csv_file)\n",
    "        except FileNotFoundError:\n",
    "            # Fallback for Kaggle environment\n",
    "            if \"kaggle\" in str(csv_file) or not Path(csv_file).exists():\n",
    "                # Try alternative path\n",
    "                alt_path = Path(\"/kaggle/input/fashion-mnist\") / Path(csv_file).name\n",
    "                if alt_path.exists():\n",
    "                    csv_file = str(alt_path)\n",
    "                    data = pd.read_csv(csv_file)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"Could not find CSV file: {csv_file}\")\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        self.labels = torch.tensor(data.iloc[:, 0].values, dtype=torch.long)\n",
    "        # Convert pixel values from [0, 255] to [0, 1] and reshape\n",
    "        images = data.iloc[:, 1:].values.astype(\"float32\") / 255.0\n",
    "        self.images = images.reshape(-1, 28, 28)\n",
    "        self.transform = transform\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        image = self.images[idx].copy()  # Shape: (28, 28), dtype: float32, range: [0, 1]\n",
    "        \n",
    "        # Apply transforms (transforms will handle conversion to tensor and normalization)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            # Default: convert to tensor and normalize if no transform provided\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image = torch.from_numpy(image).float()\n",
    "            # Add channel dimension: (28, 28) -> (1, 28, 28)\n",
    "            if image.dim() == 2:\n",
    "                image = image.unsqueeze(0)\n",
    "            # Normalize if requested\n",
    "            if self.normalize:\n",
    "                image = transforms.Normalize(mean=[0.2860], std=[0.3530])(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. DATA AUGMENTATION AND TRANSFORMS\n",
    "# ============================================================================\n",
    "class ToPILImageWrapper:\n",
    "    \"\"\"Wrapper to convert numpy array to PIL Image\"\"\"\n",
    "    def __call__(self, img):\n",
    "        from PIL import Image\n",
    "        if isinstance(img, np.ndarray):\n",
    "            # Convert to uint8 and then to PIL Image\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "            if img.ndim == 2:\n",
    "                return Image.fromarray(img, mode='L')\n",
    "            elif img.ndim == 3 and img.shape[0] == 1:\n",
    "                return Image.fromarray(img.squeeze(0), mode='L')\n",
    "        return img\n",
    "\n",
    "def get_transforms(augment: bool = False):\n",
    "    \"\"\"\n",
    "    Get data transformation pipeline.\n",
    "    \n",
    "    Args:\n",
    "        augment: Whether to apply data augmentation (for training)\n",
    "    \n",
    "    Returns:\n",
    "        Transform function or Compose object\n",
    "    \"\"\"\n",
    "    if augment:\n",
    "        # Training transforms with augmentation\n",
    "        # Note: Horizontal flip removed as it may not be appropriate for clothing items\n",
    "        return transforms.Compose([\n",
    "            ToPILImageWrapper(),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.2860], std=[0.3530])\n",
    "        ])\n",
    "    else:\n",
    "        # Validation/test transforms (no augmentation) - simple normalization\n",
    "        def transform_func(img):\n",
    "            if isinstance(img, np.ndarray):\n",
    "                img = torch.from_numpy(img.copy()).float()\n",
    "            elif isinstance(img, torch.Tensor):\n",
    "                img = img.clone().float()\n",
    "            # Ensure channel dimension exists\n",
    "            if img.dim() == 2:\n",
    "                img = img.unsqueeze(0)\n",
    "            elif img.dim() == 3 and img.shape[0] != 1:\n",
    "                # If shape is (H, W, C), convert to (C, H, W)\n",
    "                img = img.permute(2, 0, 1)\n",
    "            # Normalize\n",
    "            return transforms.Normalize(mean=[0.2860], std=[0.3530])(img)\n",
    "        return transform_func\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "def load_data(config: Config) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Load and prepare datasets with train/validation/test splits.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_loader, val_loader, test_loader)\n",
    "    \"\"\"\n",
    "    # Try to load train data\n",
    "    try:\n",
    "        if config.TRAIN_CSV.exists():\n",
    "            train_path = config.TRAIN_CSV\n",
    "        else:\n",
    "            # Fallback: try Kaggle path\n",
    "            train_path = \"/kaggle/input/fashion-mnist/fashion-mnist_train.csv\"\n",
    "            if not Path(train_path).exists():\n",
    "                # If train file doesn't exist, use test file for both (backward compatibility)\n",
    "                print(\"Warning: Train file not found. Using test file for training.\")\n",
    "                train_path = config.TEST_CSV if config.TEST_CSV.exists() else \"/kaggle/input/fashion-mnist/fashion-mnist_test.csv\"\n",
    "    except:\n",
    "        train_path = \"/kaggle/input/fashion-mnist/fashion-mnist_train.csv\"\n",
    "    \n",
    "    # Try to load test data\n",
    "    try:\n",
    "        if config.TEST_CSV.exists():\n",
    "            test_path = config.TEST_CSV\n",
    "        else:\n",
    "            test_path = \"/kaggle/input/fashion-mnist/fashion-mnist_test.csv\"\n",
    "    except:\n",
    "        test_path = \"/kaggle/input/fashion-mnist/fashion-mnist_test.csv\"\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = FashionMNISTDataset(\n",
    "        train_path, \n",
    "        transform=get_transforms(augment=True),\n",
    "        normalize=False  # Normalization handled in transforms\n",
    "    )\n",
    "    \n",
    "    test_dataset = FashionMNISTDataset(\n",
    "        test_path,\n",
    "        transform=get_transforms(augment=False),\n",
    "        normalize=False\n",
    "    )\n",
    "    \n",
    "    # Split training data into train and validation\n",
    "    val_size = int(len(train_dataset) * config.VALIDATION_SPLIT)\n",
    "    train_size = len(train_dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY\n",
    "    )\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Load data\n",
    "train_loader, val_loader, test_loader = load_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-01T05:58:11.484010Z",
     "iopub.status.busy": "2025-05-01T05:58:11.483700Z",
     "iopub.status.idle": "2025-05-01T05:58:11.495685Z",
     "shell.execute_reply": "2025-05-01T05:58:11.494927Z",
     "shell.execute_reply.started": "2025-05-01T05:58:11.483987Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to fashion_mnist_cnn.pth\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. IMPROVED CNN MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class ImprovedCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Improved CNN architecture for Fashion-MNIST classification.\n",
    "    Features:\n",
    "    - Batch normalization after each conv layer\n",
    "    - Dropout for regularization\n",
    "    - Residual connections for better gradient flow\n",
    "    - Optimized architecture for 28x28 images\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate: float = 0.5):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        \n",
    "        # First conv block: 28x28 -> 14x14\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Second conv block: 14x14 -> 7x7\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Third conv block: 7x7 -> 3x3\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.bn_fc = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # First block\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn5(self.conv5(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn_fc(self.fc1(x)))\n",
    "        x = self.dropout_fc(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_model_size(self) -> int:\n",
    "        \"\"\"Return the number of trainable parameters\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. VISUALIZATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def show_sample_images(\n",
    "    dataloader: DataLoader, \n",
    "    class_names: List[str], \n",
    "    count: int = 9\n",
    ") -> None:\n",
    "    \"\"\"Display sample images from the dataset\"\"\"\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    images, labels = next(dataloader_iter)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(count, len(images))):\n",
    "        img = images[i].squeeze().cpu().numpy()\n",
    "        # Denormalize for display\n",
    "        img = img * 0.3530 + 0.2860\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f\"Label: {labels[i].item()} - {class_names[labels[i].item()]}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Images from Dataset\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_class_distribution(dataloader: DataLoader, class_names: List[str]) -> None:\n",
    "    \"\"\"Plot class distribution\"\"\"\n",
    "    all_labels = []\n",
    "    for _, labels in dataloader:\n",
    "        all_labels.extend(labels.numpy())\n",
    "    \n",
    "    label_counts = np.bincount(all_labels)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Pie chart\n",
    "    ax1.pie(label_counts, labels=class_names, autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title(\"Class Distribution (Pie Chart)\")\n",
    "    \n",
    "    # Bar chart\n",
    "    ax2.bar(range(len(class_names)), label_counts)\n",
    "    ax2.set_xlabel(\"Class\")\n",
    "    ax2.set_ylabel(\"Count\")\n",
    "    ax2.set_title(\"Class Distribution (Bar Chart)\")\n",
    "    ax2.set_xticks(range(len(class_names)))\n",
    "    ax2.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show sample images and distribution\n",
    "print(\"Sample Images from Training Set:\")\n",
    "show_sample_images(train_loader, config.CLASS_NAMES, count=9)\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "plot_class_distribution(train_loader, config.CLASS_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. TRAINING AND EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def evaluate_model(\n",
    "    model: nn.Module, \n",
    "    dataloader: DataLoader, \n",
    "    device: torch.device,\n",
    "    return_predictions: bool = False\n",
    ") -> Tuple[float, Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataloader.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (accuracy, predictions, true_labels) if return_predictions=True,\n",
    "        else (accuracy, None, None)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            if return_predictions:\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    accuracy = 100.0 * correct / total\n",
    "    \n",
    "    if return_predictions:\n",
    "        return accuracy, np.array(all_preds), np.array(all_labels)\n",
    "    return accuracy, None, None\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping utility to stop training when validation loss stops improving\"\"\"\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0.001, mode: str = 'max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, score: float) -> bool:\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif self.mode == 'max':\n",
    "            if score < self.best_score + self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "        else:  # mode == 'min'\n",
    "            if score > self.best_score - self.min_delta:\n",
    "                self.counter += 1\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.counter = 0\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100.0 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    config: Config,\n",
    "    device: torch.device\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and learning rate scheduling.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    # Initialize optimizer and criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.5, \n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=config.EARLY_STOPPING_PATIENCE,\n",
    "        min_delta=config.MIN_DELTA,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Training model with {model.get_model_size():,} parameters\")\n",
    "    print(f\"{'Epoch':<8} {'Train Loss':<12} {'Train Acc':<12} {'Val Loss':<12} {'Val Acc':<12} {'LR':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_acc, _, _ = evaluate_model(model, val_loader, device)\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_acc)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            if config.SAVE_BEST_MODEL:\n",
    "                torch.save(\n",
    "                    best_model_state,\n",
    "                    config.MODEL_SAVE_PATH / \"best_model.pth\"\n",
    "                )\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"{epoch+1:<8} {train_loss:<12.4f} {train_acc:<12.2f} {val_loss:<12.4f} {val_acc:<12.2f} {current_lr:<10.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_acc):\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nLoaded best model with validation accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. INITIALIZE AND TRAIN MODEL\n",
    "# ============================================================================\n",
    "model = ImprovedCNN(dropout_rate=config.DROPOUT_RATE).to(device)\n",
    "print(f\"Model initialized with {model.get_model_size():,} trainable parameters\\n\")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, train_loader, val_loader, config, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 9. VISUALIZE TRAINING HISTORY\n",
    "# ============================================================================\n",
    "def plot_training_history(history: Dict[str, List[float]]) -> None:\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[1, 0].plot(epochs, history['learning_rate'], 'g-', linewidth=2)\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy difference (overfitting indicator)\n",
    "    acc_diff = [val - train for train, val in zip(history['train_acc'], history['val_acc'])]\n",
    "    axes[1, 1].plot(epochs, acc_diff, 'purple', linewidth=2)\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\n",
    "    axes[1, 1].set_title('Validation - Training Accuracy (Overfitting Indicator)', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy Difference (%)')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 10. EVALUATE ON TEST SET\n",
    "# ============================================================================\n",
    "test_acc, test_preds, test_labels = evaluate_model(\n",
    "    model, test_loader, device, return_predictions=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Test Set Results\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(\n",
    "    test_labels, \n",
    "    test_preds, \n",
    "    target_names=config.CLASS_NAMES,\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 11. CONFUSION MATRIX\n",
    "# ============================================================================\n",
    "def plot_confusion_matrix(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    class_names: List[str]\n",
    ") -> None:\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate per-class accuracy\n",
    "    class_accuracies = cm.diagonal() / cm.sum(axis=1) * 100\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for i, (class_name, acc) in enumerate(zip(class_names, class_accuracies)):\n",
    "        print(f\"  {class_name:<20}: {acc:.2f}%\")\n",
    "\n",
    "plot_confusion_matrix(test_labels, test_preds, config.CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 12. VISUALIZE PREDICTIONS\n",
    "# ============================================================================\n",
    "def show_predictions(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    class_names: List[str],\n",
    "    device: torch.device,\n",
    "    count: int = 16,\n",
    "    show_errors_only: bool = False\n",
    ") -> None:\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    images_shown = 0\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            \n",
    "            for i in range(images.size(0)):\n",
    "                if images_shown >= count:\n",
    "                    break\n",
    "                \n",
    "                true_label = labels[i].item()\n",
    "                pred_label = preds[i].item()\n",
    "                confidence = probabilities[i][pred_label].item() * 100\n",
    "                \n",
    "                # Skip if showing errors only and prediction is correct\n",
    "                if show_errors_only and true_label == pred_label:\n",
    "                    continue\n",
    "                \n",
    "                img = images[i].cpu().squeeze().numpy()\n",
    "                # Denormalize for display\n",
    "                img = img * 0.3530 + 0.2860\n",
    "                img = np.clip(img, 0, 1)\n",
    "                \n",
    "                axes[images_shown].imshow(img, cmap='gray')\n",
    "                color = 'green' if true_label == pred_label else 'red'\n",
    "                title = f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {confidence:.1f}%\"\n",
    "                axes[images_shown].set_title(title, color=color, fontsize=9)\n",
    "                axes[images_shown].axis('off')\n",
    "                \n",
    "                images_shown += 1\n",
    "            \n",
    "            if images_shown >= count:\n",
    "                break\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(images_shown, count):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Model Predictions on Test Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show correct predictions\n",
    "print(\"Sample Correct Predictions:\")\n",
    "show_predictions(model, test_loader, config.CLASS_NAMES, device, count=16, show_errors_only=False)\n",
    "\n",
    "# Show incorrect predictions if any\n",
    "print(\"\\nIncorrect Predictions (if any):\")\n",
    "show_predictions(model, test_loader, config.CLASS_NAMES, device, count=16, show_errors_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 13. SAVE MODEL AND FINAL SUMMARY\n",
    "# ============================================================================\n",
    "# Save final model\n",
    "final_model_path = config.MODEL_SAVE_PATH / \"fashion_mnist_cnn_final.pth\"\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "\n",
    "# Save training history\n",
    "import json\n",
    "history_path = config.MODEL_SAVE_PATH / \"training_history.json\"\n",
    "with open(history_path, 'w') as f:\n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    json_history = {k: [float(v) for v in vals] for k, vals in history.items()}\n",
    "    json.dump(json_history, f, indent=2)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Model Parameters: {model.get_model_size():,}\")\n",
    "print(f\"Best Validation Accuracy: {max(history['val_acc']):.2f}%\")\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"Total Epochs Trained: {len(history['train_loss'])}\")\n",
    "print(f\"Final Learning Rate: {history['learning_rate'][-1]:.6f}\")\n",
    "print(f\"{'='*70}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7236637,
     "sourceId": 11539334,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
